name: Ejecución Programada del Scraper

on:
  schedule:
    # Ejecutar a las 8:00, 14:00 y 20:00 UTC (9:00, 15:00 y 21:00 en horario de verano español)
    - cron: '0 6,12,18 * * *'
  workflow_dispatch:  # Permite ejecutar manualmente

jobs:
  scrape-and-upload:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout código
      uses: actions/checkout@v3
    
    - name: Configurar Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Instalar dependencias
      run: |
        echo "=== Información del sistema ==="
        uname -a
        python --version
        pip --version
        
        echo "\n=== Contenido del directorio ==="
        ls -la
        
        echo "\n=== Contenido de requirements_competitors.txt ==="
        cat requirements_competitors.txt
        
        echo "\n=== Instalando dependencias ==="
        python -m pip install --upgrade pip
        pip install newspaper3k==0.2.8
        pip install -r requirements_competitors.txt
        pip install gdown
        
        echo "\n=== Dependencias instaladas ==="
        pip list
    
    - name: Crear directorio de salida
      run: |
        # Solo creamos el directorio de salida
        echo "=== Creando directorio de salida ==="
        mkdir -p output/competitors
        
        # Verificar directorio
        echo "\n=== Directorio de salida creado: ==="
        ls -la output/
        
    - name: Ejecutar scraper
      env:
        GOOGLE_DRIVE_FOLDER_ID: "1wqN9FUUlovaCZLKTdgp1sSZjjud57sFN"  # ID de la carpeta de Google Drive
      run: |
        # Configuración inicial
        set -e  # Salir en caso de error
        
        echo "=== INICIO DE EJECUCIÓN ==="
        echo "Directorio de trabajo: $(pwd)"
        
        # Crear directorio de salida
        mkdir -p output/competitors
        
        # Instalar gdown si no está instalado
        if ! command -v gdown &> /dev/null; then
          echo "Instalando gdown..."
          pip install gdown --upgrade
        fi
        
        # Ejecutar el script principal
        echo "Ejecutando el script de scraping..."
        python automation/run_automation.py || {
          echo "Error al ejecutar el script de scraping";
          exit 1;
        }
        
        # Comprimir resultados
        echo "Comprimiendo resultados..."
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        ARCHIVE_NAME="news_export_${TIMESTAMP}.tar.gz"
        
        # Verificar si hay archivos para comprimir
        if [ $(find output/competitors -name "*.csv" | wc -l) -gt 0 ]; then
          tar -czf "$ARCHIVE_NAME" -C output/competitors .
          echo "Archivo comprimido creado: $ARCHIVE_NAME"
          
          # Subir a Google Drive
          echo "Subiendo a Google Drive..."
          if [ -n "$GOOGLE_DRIVE_FOLDER_ID" ]; then
            gdown upload --folder "$GOOGLE_DRIVE_FOLDER_ID" "$ARCHIVE_NAME" || \
              echo "Advertencia: No se pudo subir a Google Drive"
          else
            echo "Advertencia: No se configuró GOOGLE_DRIVE_FOLDER_ID"
          fi
        else
          echo "No se encontraron archivos CSV para comprimir"
        fi
    
    - name: Configurar Git
      run: |
        git config --global user.name 'GitHub Actions'
        git config --global user.email 'actions@github.com'
    
    - name: Commit y Push de logs
      run: |
        git add logs/
        git diff --quiet && git diff --staged --quiet || \
        (git commit -m "Actualización automática de logs" && \
         git push origin $GITHUB_REF) || echo "No hay cambios en los logs"
